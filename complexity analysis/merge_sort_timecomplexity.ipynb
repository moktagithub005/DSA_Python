{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b4460e",
   "metadata": {},
   "source": [
    "Understanding Merge Sort and Its Time Complexity\n",
    "\n",
    "Merge Sort is a highly efficient, comparison-based sorting algorithm. It's a classic example of a Divide and Conquer algorithm. The core idea is to break down a list into several sub-lists until each sub-list consists of a single element (which is by definition sorted). Then, these sub-lists are repeatedly merged to produce new sorted sub-lists until there is only one sorted list remaining.\n",
    "\n",
    "Merge Sort Algorithm Steps:\n",
    "Divide: Divide the unsorted list into n sub-lists, each containing one element.\n",
    "\n",
    "Conquer (Recursion): Recursively sort each sub-list. Since a single element list is considered sorted, this step essentially involves no work until the lists are split down to single elements.\n",
    "\n",
    "Combine (Merge): Repeatedly merge sub-lists to produce new sorted sub-lists until there is only one sorted list remaining. This merging step is crucial and where the primary work happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5535e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    # Base case: If the array has 0 or 1 element, it's already sorted\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    # Step 1: Divide the array into two halves\n",
    "    mid = len(arr) // 2\n",
    "    left_half = arr[:mid]\n",
    "    right_half = arr[mid:]\n",
    "\n",
    "    # Step 2: Recursively sort the two halves\n",
    "    sorted_left = merge_sort(left_half)\n",
    "    sorted_right = merge_sort(right_half)\n",
    "\n",
    "    # Step 3: Merge the sorted halves\n",
    "    return merge(sorted_left, sorted_right)\n",
    "\n",
    "def merge(left, right):\n",
    "    result = []\n",
    "    i = 0  # pointer for left array\n",
    "    j = 0  # pointer for right array\n",
    "\n",
    "    # Compare elements from both arrays and add the smaller one to the result\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "\n",
    "    # Add any remaining elements from the left array (if any)\n",
    "    while i < len(left):\n",
    "        result.append(left[i])\n",
    "        i += 1\n",
    "\n",
    "    # Add any remaining elements from the right array (if any)\n",
    "    while j < len(right):\n",
    "        result.append(right[j])\n",
    "        j += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example Usage:\n",
    "# my_array = [38, 27, 43, 3, 9, 82, 10]\n",
    "# sorted_array = merge_sort(my_array)\n",
    "# print(sorted_array) # Output: [3, 9, 10, 27, 38, 43, 82]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3244c9",
   "metadata": {},
   "source": [
    "Time Complexity Analysis of Merge Sort\n",
    "To analyze the time complexity of Merge Sort, we use a recurrence relation. Let T(n) be the time taken to sort an array of size n.\n",
    "\n",
    "Based on the algorithm's steps:\n",
    "\n",
    "Dividing the array: This involves calculating the middle index and splitting the array. This is generally a constant time operation, let's denote it as k_1. In some implementations, slicing might take O(n) time, but conceptually for recurrence, we consider the setup for recursion. However, for a more precise analysis, preparing the sub-arrays (e.g., copying elements) can take O(n) time. The provided text simplifies this initial setup to a constant k_1.\n",
    "\n",
    "Recursively sorting two halves: We make two recursive calls to merge_sort, each on an array of size n/2. This contributes 2T(n/2) to the total time.\n",
    "\n",
    "Merging the sorted halves: The merge function takes two sorted sub-arrays and merges them into a single sorted array. In the merge function, we iterate through both left and right arrays, comparing elements and adding them to the result array. In the worst case, we will go through all n elements. This operation takes linear time, which is proportional to the total number of elements in the two sub-arrays being merged (which sum up to n). Let's denote this as k_2n, where k_2 is a constant.\n",
    "\n",
    "Combining these, the recurrence relation for Merge Sort is:\n",
    "\n",
    "T(n)=k_1+2T(n/2)+k_2n\n",
    "\n",
    "As the constant k_1 will become insignificant compared to k_2n for large n, we can simplify it to:\n",
    "\n",
    "T(n)=2T(n/2)+kn (where k absorbs k_1 and k_2 into a single constant proportional to n)\n",
    "\n",
    "Now, let's solve this recurrence relation using the substitution method or the recursion tree method. The provided text demonstrates a similar approach, effectively expanding the recurrence.\n",
    "\n",
    "Expansion of the Recurrence Relation:\n",
    "\n",
    "Level 0: T(n)=kn+2T(n/2)\n",
    "\n",
    "Work at this level: kn\n",
    "\n",
    "Level 1: For each T(n/2), we expand it as k(n/2)+2T(n/4). Since there are two such terms, the total contribution from this level is 2\n",
    "times(k(n/2)+2T(n/4))=kn+4T(n/4).\n",
    "\n",
    "Work at this level: kn\n",
    "\n",
    "Level 2: Expanding T(n/4), we get k(n/4)+2T(n/8). Since there are four such terms, the total contribution is 4\n",
    "times(k(n/4)+2T(n/8))=kn+8T(n/8).\n",
    "\n",
    "Work at this level: kn\n",
    "\n",
    "We can observe a pattern here: at each level of recursion, the total work done for merging is kn.\n",
    "\n",
    "This process continues until the sub-arrays become of size 1 (base case: T(1)=K, a constant amount of work for a single element).\n",
    "The depth of the recursion tree, or the number of levels, can be found by determining how many times we can divide n by 2 until it reaches 1:\n",
    "\n",
    "n/2 \n",
    "x\n",
    " =1\n",
    "implies2 \n",
    "x\n",
    " =n\n",
    "impliesx=\n",
    "log_2n\n",
    "\n",
    "So, there are approximately \n",
    "log_2n levels.\n",
    "\n",
    "Summing the work done at each level:\n",
    "\n",
    "Total Work T(n)=(\n",
    "textWorkatLevel0)+(\n",
    "textWorkatLevel1)+\n",
    "ldots+(\n",
    "textWorkatLevel\n",
    "log_2nâˆ’1)+(\n",
    "textWorkatBaseCase)\n",
    "\n",
    "T(n)=kn+kn+kn+\n",
    "ldots+kn (for \n",
    "log_2n times) + (work for base cases)\n",
    "\n",
    "The total work for the kn part across all levels is kn\n",
    "times\n",
    "log_2n.\n",
    "The base case involves n single-element arrays, each taking constant time, contributing n\n",
    "timesK which is O(n). However, in terms of overall dominance, kn\n",
    "log_2n will be the leading term.\n",
    "\n",
    "Therefore, T(n)=O(n\n",
    "logn).\n",
    "\n",
    "Important Considerations:\n",
    "\n",
    "Best, Average, and Worst Case: One of the significant advantages of Merge Sort is that its time complexity remains O(n\n",
    "logn) in all cases (best, average, and worst). This is because the algorithm always divides the array into two halves and performs the merge operation on them, regardless of whether the input array is already sorted, reverse sorted, or randomly ordered. The number of comparisons and merges remains consistent.\n",
    "\n",
    "Comparison with O(n \n",
    "2\n",
    " ) Algorithms:\n",
    "\n",
    "Algorithms like Bubble Sort, Insertion Sort, and Selection Sort have a worst-case time complexity of O(n \n",
    "2\n",
    " ).\n",
    "\n",
    "For a large input size, say n=10 \n",
    "6\n",
    "  (1 million), an O(n \n",
    "2\n",
    " ) algorithm would perform (10 \n",
    "6\n",
    " ) \n",
    "2\n",
    " =10 \n",
    "12\n",
    "  operations. This is a massive number, leading to very long execution times (as demonstrated by the 24-minute example for Bubble Sort).\n",
    "\n",
    "For the same n=10 \n",
    "6\n",
    " , an O(n\n",
    "logn) algorithm would perform 10 \n",
    "6\n",
    " \n",
    "times\n",
    "log_2(10 \n",
    "6\n",
    " ) operations. Since \n",
    "log_2(10 \n",
    "6\n",
    " ) is approximately 20 (as 2 \n",
    "10\n",
    " \n",
    "approx10 \n",
    "3\n",
    " , so 2 \n",
    "20\n",
    " \n",
    "approx(10 \n",
    "3\n",
    " ) \n",
    "2\n",
    " =10 \n",
    "6\n",
    " ), the operations would be around 10 \n",
    "6\n",
    " \n",
    "times20=2\n",
    "times10 \n",
    "7\n",
    " . This is a significantly smaller number of operations, leading to much faster execution times (seconds or less).\n",
    "\n",
    "This vast difference in the number of operations highlights why understanding and choosing algorithms with better time complexity (like Merge Sort over Bubble Sort for large datasets) is crucial for efficient software development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6bd1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5107b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3f3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1a01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4aadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
